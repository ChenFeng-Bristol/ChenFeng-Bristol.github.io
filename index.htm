<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Chen Feng</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
</head>

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
                        <div class="col-md-3">
                           	
						<br>
						<br>
						    <p><img src="Chen.jpg" width="90%" height="20%" class="center" /></p>
                        </div>
                        <div class="col-md-6">
                            <div class="title">
							<br>
							<br>
                                <h1>Chen Feng </h1>
								<b>冯 &nbsp;&nbsp;&nbsp 晨</b> 
								<br>
								<br>

								<b><font size="+1">PhD Student</font></b><br>

								Visual Information Laboratory<br>
		
								Electrical, Electronic and Mechanical Engineering<br>
								University of Bristol<br>
								<br>
								1.23, 1 Cathedral Square<br>
                                Bristol BS1 5DD, United Kingdom<br>
                                <a href='mailto:chen.feng@bristol.ac.uk'>chen.feng@bristol.ac.uk</a><br>
								
								<ul class="list-inline list-social-icons mb-0">
								<br>
                                    <li class="list-inline-item">
                                        <a href="https://scholar.google.com/citations?hl=en&user=kWY2QHoAAAAJ">
                                            <span class="fa-3x">
                                                <i class="ai ai-google-scholar-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                    <li class="list-inline-item">
                                        <a href="https://www.linkedin.com/in/chen-feng-721442261/">
                                            <span class="fa-3x">
												<i class="fab fa-linkedin"></i>
                                            </span>
                                        </a>
                                    </li>
									<li class="list-inline-item">
                                        <a href="https://github.com/ChenFeng-Bristol">
                                            <span class="fa-3x">
                                                <i class="fab fa-github-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                    <li class="list-inline-item">
                                        <a href="./CV_ChenFeng_PhD_Univeristyofbristol.pdf">
                                            <span class="fa-3x">
                                                <i class="ai ai-cv-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <div class="col-md-3">
						<div class="title">
							<br>
							<br>
                                <p><a href='https://www.bristol.ac.uk'><img src="uob-logo.svg" width="90%" height="30%" alt=""></a></p>
								<br>
								<p><a href='https://www.bristol.ac.uk/vision-institute'><img src="bvilogo.png" width="85%" height="30%" alt=""></a></p>		
								<br>
								<p><a href='https://vilab.blogs.bristol.ac.uk'><img src="VILogo.jpg" width="95%" height="30%" alt=""></a></p>
                            </div>
						
                        </div>
                    </div>
                </div>
            </div>
			<br>
			<div class="container-md">
				<h2>About</h2>
				I am a final year PhD student in <a href="https://vilab.blogs.bristol.ac.uk">Visual Information Laboratory</a> at <a href="http://www.bris.ac.uk/">University of Bristol</a> under the supervision of <a href="https://david-bull.github.io/">Prof. David Bull</a> and <a href="https://seis.bristol.ac.uk/~eexfz/index.htm">Dr Aaron Zhang</a>, funded by the <a href="https://www.amazon.science/research-awards/program-updates/79-amazon-research-awards-recipients-announced">Amazon Research Awards 2022</a>. My main research interests are low-level computer vision including deep video quality assessment, deep video compression and restoration, and subjective video quality assessment.<br>
				<br>
				I obtained an MSc (Distinction) in image and video communication and signal processing from the <a href="http://www.bris.ac.uk/">University of Bristol</a>, where I researched on <a href="CNN-PP/index.htm">Video Compression with CNN-based Post Processing</a> in the <a href="https://vilab.blogs.bristol.ac.uk">Visual Information Laboratory</a> under the supervision of <a href="https://david-bull.github.io/">Prof. David Bull</a>. I completed a BEng in Measuring and Control Technology at the School of Automation and Electrical Engineering at the University of Science and Technology Beijing under the supervision of Prof. Qing Li.<br>
				<br>
				Currently, I am working as an <b>Applied Scientist Intern</b> at Video Coding Standards Research Team of <b>Amazon Prime Video</b> in Seattle, US.
			</div>
			
			<hr />
			
			 <div class="container-md">
                      <h2 id="projects">Main Awards</h2>
                      <ul>
			      	<li><b>Third Place</b> in <a HREF="https://www.cvlai.net/aim/2024/">the ECCV 2024 AIM Challenge in Efficient Video Super-Resolution</a> (1st in PSNR, 3rd in VMAF and 1st in efficiency)</li>
				<li><b>Winner</b> in <a HREF="https://faculty.ustc.edu.cn/lil1/en/zdylm/991871/list/index.htm">IEEE MMSP 2024 3rd Practical End-to-end Image/Video Compression Challenge</a> (Track 2)</li>
				<li><b>First Place</b> in <a HREF="https://www.compression.cc/leaderboard/video_perceptual/test/">DCC 2024 6th Challenge on Learned Image Compression</a> (CLIC, NR VQA Track)</li>
				<li><b>First Prize</b> in <a HREF="https://sites.google.com/view/wacv2023-workshop-quality-va/competition?authuser=0">IEEE/CVF WACV 2023 HDR VQA Grand Challenge</a> (No Reference Track)</li>
                      </ul>
                  </div>
			<hr />
            <div class="container-md">
                <h2 id="projects">News & Activities</h2>
                <ul>
					<li><b>Oct 2024:</b> The paper on "<A HREF="https://arxiv.org/abs/2406.00212">MVAD: A Multiple Visual Artifact Detector for Video Streaming</a>" has been accepted by <a HREF="https://wacv2025.thecvf.com/">WACV 2025<a>.
					<li><b>Sep 2024:</b> Contributed to our BVI-SR team's participation in the <a HREF="https://www.cvlai.net/aim/2024/">the 5th AIM Challenge in Efficient Video Super-Resolution</a> in ECCV 2024, and our submission ranked <font style="color:red">THIRD</font> (1st in PSNR, 3rd in VMAF and 1st in efficiency).
					<li><b>Sep 2024:</b> Contributed to our BVI-VC team's participation in the  <a HREF="https://faculty.ustc.edu.cn/lil1/en/zdylm/991871/list/index.htm">the 3rd practical end-to-end image/video compression challenge</a> in IEEE MMSP 2024, and our team is the <font style="color:red">WINNER</font> in Track 2.
					<li><b>Aug 2024:</b> As an equal contributor and first author, our paper titled "<A HREF="https://arxiv.org/abs/2405.08621">RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content</a>" has been accepted by <a HREF="https://cvlai.net/aim/2024/">ECCV Workshop in Advances in Image Manipulation (AIM) 2024</a>.
					<li><b>Jul 2024:</b> One co-author paper on "<A HREF="https://arxiv.org/abs/2404.09571">MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution</a>" has been accepted by <a HREF="https://eccv2024.ecva.net/">ECCV 2024</a>.
					<li><b>Jul 2024:</b> Happy to start a summer internship as an <b>Applied Scientist Intern</b> at <b>Amazon Prime Video</b> in Seattle, US. </a>
					<li><b>May 2024:</b> Participated the <a HREF="https://www.compression.cc/leaderboard/video_perceptual/test/">6th Challenge on Learned Image Compression</a> in Data Compression Conference 2024. Our submission ranked <font style="color:red">FIRST</font> on the leaderboard in the Video Perception (NR VQA) track.
					<li><b>Feb 2024:</b> <font style="color:red">TWO</font> first author papers and <font style="color:red">ONE</font> second author have been accepted by <a HREF="https://2024.picturecodingsymposium.org/">Picture Coding Symposium (PCS) 2024</a>, including <A HREF="https://arxiv.org/abs/2312.08859">BVI-Artefact database</A>, <A HREF="https://arxiv.org/abs/2312.08864">RankDVQA-mini</A>, and <A HREF="https://arxiv.org/abs/2312.12317">VQA for UGC transcoding</A>.
					<li><b>Jan 2023:</b> Started working as a Teaching Assistant for MSc unit <a HREF="https://www.bristol.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=22%2F23&unitCode=COMSM0129">Augmenting the Real World (AR)</a>.
					<li><b>Oct 2023:</b> Our paper on "<A HREF="https://arxiv.org/abs/2202.08595">RankDVQA: Deep VQA based on Ranking-inspired Hybrid Training</a>" has been accepted by <a HREF="https://wacv2024.thecvf.com/">IEEE/CVF WACV 2024<a>.
					<li><b>Apr 2023:</b> My PhD is funded by <a HREF="amzn.to/ara-fall-2022">Amazon Research Award</a> to conduct research on video quality assessment.					
					<li><b>Jan 2023:</b> Participated the <a HREF="https://sites.google.com/view/wacv2023-workshop-quality-va/competition/">HDR Video Quality Measurement Grand Challenge</a> in IEEE/CVF WACV 2023, organised by Amazon Prime Video. Our submission ranked <font style="color:red">the FIRST</font> in the no reference VQA track. <a href="./WACV.pdf">(award letter)</a></li>
				</ul>
				<p><a id="showOlderNews" href="#olderNews">[Older news and activities]</a></p>
				<div id="olderNews">
				<a name="olderNews" />
				<ul>
					<li><b>Jan 2023:</b> Started working as a Teaching Assistant for MSc unit <a HREF="https://www.bristol.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=22%2F23&unitCode=COMSM0127">Immersive Interaction and Audio Design (VR)</a>.
					<li><b>June 2022:</b> Participated the <a HREF="http://compression.cc/">Challenge on Learned Image Compression (CLIC)</a> in IEEE/CVF CVPR 2022, and ranks <font style="color:red">top six</font> in the video track.
					<li><b>May 2022:</b> Our team participated the <a HREF="https://mailchi.mp/ieee-cas.org/ieee-cass-seasonal-school-on-technology-and-agribusiness-391034?e=2bef362975">Grand Challenge on Neural Network-based Video Coding</a> in IEEE ISCAS 2022, and ranks the <font style="color:red">Second Place</font> in the hybrid track.
					<li><b>May 2022:</b> Paper "<a href="ViSTRA3/index.htm">ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing</a>" was accepted by ISCAS 2022.</li>				
					<li><b>Jan 2021:</b> Paper "<a href="CNN-PP/index.htm">Video Compression with CNN-based Post Processing.</a>" was accepted by IEEE MultiMedia.
					<li><b>Jan 2021:</b> Started my PhD study at University of Bristol.</li>	

				</ul>
            </div>
<!-- 			
            <hr />
			            <div class="container-md">
                <h2>Research Areas and Projects</h2> 
				
				 <div class="row">
                    <div class="row vertical-align">
				<div class="col-md-6">
				<ul>
                    <li><b>Video Quality Assessment</b></li>
					<ul>
						<li><A HREF="RankDVQA/index.htm">RankDVQA: Deep VQA based on Ranking-inspired Hybrid Training</a>
						<li><a href="DVQM-HT/index.htm">Deep VQA based on a Novel Hybrid Training Methodology</a></li>
						<li><a href="BVQM-HT/index.htm"> A Blind Quality Metric based on Hybrid Training for HDR VQA</a></li>
                    </ul>
                </ul>
				</div>
				 <div class="col-md-6">
				<ul>
                    <li><b>Deep Video Coding</b></li>
					<ul>
						<li><a href="ViSTRA3/index.htm">ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing.</a></li>
                    
						<li><a href="CNN-PP/index.htm">Video Compression with CNN-based Post Processing.</a></li>
                    </ul>
                </ul>
				</div>
				
 -->
				<hr />			
			            <div class="container-md">
                <h2>Signature Research Projects</h2>
			

<br>
			<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">WACV 2025</abbr></div>
        <img src="./figures/MVAD_framework6.jpg" width="100%" \="">
    </p></td>
	
    
    <td class="detail">   
    <b id="papertitle"> MVAD: A Multiple Visual Artifact Detector for Video Streaming </b>
    <br>
	<em><font style="color:blue">"The first multi-artifact detection
framework without relying on VQA!"</font></em>
	<br>
  <u>Chen Feng</u>, Duolikun Danier, Fan Zhang and David Bull
    <br>
    <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2025
    <br>
    <a href="https://arxiv.org/pdf/2406.00212" target="_blank" style="font-style:normal;">[arXiv]</a>
	<a href="https://chenfeng-bristol.github.io/MVAD/index.htm" target="_blank" style="font-style:normal;">[project]</a> 	
    </td>
    </tr>
    </tbody></table>
</div>
<br>
	
			
	


<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td class="thumbs"><p>
        </p><div class="col-sm-3 abbr"><abbr class="badge">WACV 2024</abbr></div>
        <img src="./figures/rankdvqa.png" width="100%" \="">
    <p></p></td>
    <td class="detail">   
    <b id="papertitle"> RankDVQA: Deep VQA based on Ranking-inspired Hybrid Training </b>
    <br>
	<em><font style="color:blue">"The first deep VQA model optimised through ranking-based training!"</font></em>
	<br>
    <u>Chen Feng</u>, Duolikun Danier, Fan Zhang, David Bull
    <br>
    <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2024
    <br>
    <a href="https://arxiv.org/abs/2202.08595" target="_blank" style="font-style:normal;">[arXiv]</a> 
    <a href="https://chenfeng-bristol.github.io/RankDVQA/" target="_blank" style="font-style:normal;">[project]</a>  
	<a href="https://github.com/ChenFeng-Bristol/RankDVQA_release" target="_blank" style="font-style:normal;">[code]</a> 	
    </td>
    </tr>
    </tbody></table>
    <!-- <a href="https://github.com/crispianm/ST-MFNet-Mini" target="_blank"><p class="lab la-github"></p>&#32;</a>
    <a href="https://github.com/crispianm/ST-MFNet-Mini" target="_blank" style="font-style:normal;">github</a>&#8201;&#8201;&#8201; -->
<!-- </div> -->



				<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">ECCV 2024</abbr></div>
        <img src="./figures/MTKD.svg" width="100%" \="">
    </p></td>
	
    
    <td class="detail">   
    <b id="papertitle"> MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution </b>
    <br>
	<em><font style="color:blue">"A student model can outperform its teaching models through MTKD!"</font></em>
	<br>
    Yuxuan Jiang, <u>Chen Feng</u>, Fan Zhang and David Bull
    <br>
    <em>ECCV</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2404.09571" target="_blank" style="font-style:normal;">[arXiv]</a>  
<a href="https://github.com/YuxuanJJ/MTKD" target="_blank" style="font-style:normal;">[code]</a>	
    </td>
    </tr>
    </tbody></table>
</div>	

<br>
			
			<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">ECCV AIM Workshop 2024</abbr></div>
        <img src="./figures/RMViT.svg" width="100%" \="">
    </p></td>
	
    
    <td class="detail">   
    <b id="papertitle"> RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content </b>
    <br>
	<em><font style="color:blue">"The first deep VQA model based on RMT!"</font></em>
	<br>
	Tianhao Peng<sup>*</sup>,<u>Chen Feng</u><sup>*</sup>, Duolikun Danier, Fan Zhang, Benoit Vallade, Alex Mackin and David Bull
    <br>
    <em>ECCV AIM Workshop</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2405.08621" target="_blank" style="font-style:normal;">[arXiv]</a> 
<a href="https://github.com/jasminepp/RMTBVQA" target="_blank" style="font-style:normal;">[Code]</a> 	
    </td>
    </tr>
    </tbody></table>
</div>


			<br>

				<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">PCS 2024</abbr></div>
        <img src="./figures/BVI-Artefact.svg" width="100%" \="">
    </p></td>
	
    
    <td class="detail">   
    <b id="papertitle"> BVI-Artefact: An artefact detection benchmark dataset for streamed videos </b>
    <br>
	<em><font style="color:blue">"The first public database for detecting artefacts in streamed PGC videos!"</font></em>
	<br>
	<u>Chen Feng</u><sup>*</sup>, Duolikun Danier<sup>*</sup>, Fan Zhang, Alex Mackin, Andy Collins and David Bull
    <br>
    <em>PCS</em>, 2024
    <br>
    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10566356" target="_blank" style="font-style:normal;">[Paper]</a> 
	<a href="https://chenfeng-bristol.github.io/BVI-Artefact/" target="_blank" style="font-style:normal;">[Database]</a> 	
    </td>
    </tr>
    </tbody></table>
</div>


			<br>

			<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">PCS 2024</abbr></div>
        <img src="./RankDVQA-mini/framework.png" width="100%" \="">
    </p></td>
	
    
    <td class="detail">   
    <b id="papertitle"> RankDVQA-mini: Knowledge distillation-driven deep video quality assessment </b>
    <br>
	<em><font style="color:blue">"The first lightweight deep VQA achieving competitive performance!"</font></em>
	<br>
	<u>Chen Feng</u>, Duolikun Danier, Haoran Wang, Fan Zhang, Benoit Vallade, Alex Mackin and David Bull
    <br>
    <em>PCS</em>, 2024
    <br>
    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10566364" target="_blank" style="font-style:normal;">[Paper]</a> 
	<a href="https://chenfeng-bristol.github.io/RankDVQA-mini/" target="_blank" style="font-style:normal;">[Project]</a> 	
    </td>
    </tr>
    </tbody></table>
</div>


			<br>			
			
			<br>
			<div class="container-md">
    <table class="papers" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
	    <td class="thumbs"><p>
		 </p><div class="col-sm-3 abbr"><abbr class="badge">arXiv 2024</abbr></div>
        <img src="./figures/BVI-UGC.jpg" width="100%" \="">
    </p></td>
	
    
    <td class="detail"> 


			

    <b id="papertitle"> BVI-UGC: A Video Quality Database for User-Generated Content Transcoding </b>
    <br>
	<em><font style="color:blue">"The first video quality database focusing on UGC transcoding!"</font></em>
	<br>
  Zihao Qi, <u>Chen Feng</u>, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull
    <br>
    <em>arXiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2408.07171" target="_blank" style="font-style:normal;">[arXiv]</a>  
<a href="https://zihaoq1.github.io/BVI-UGC/" target="_blank" style="font-style:normal;">[Database]</a>  	
    </td>
    </tr>
    </tbody></table>
</div>


 </div>

    
            	
            <hr />
				
			
			<div class="container-md">
  <h2 id="mentorship-experience">Experience</h2>
  
  <div class="entry">
    <b>Applied Scientist Intern</b>
    <p><strong>Amazon Prime Video</strong> <span style="float: right;">Seattle, US &nbsp;&nbsp; Jul. 2024 - Present</span></p>
    <ul>
      <li>Video Coding Standards Research Team - Research on Deep Video Compression.</li>
    
    </ul>
  </div>
  
  <div class="entry">
    <b>Teaching Assistant</b>
    <p><strong>University of Bristol</strong>   <span style="float: right;">Bristol, UK &nbsp;&nbsp; Feb. 2021 - Present</span></p>
    <ul>
      <li>Image Processing and Computer Vision: supported lectures and independently delivered tutorial sessions</li>
      <li>Immersive Interaction and Audio Design (VR Development): designed and supported lab sessions (Unity)</li>
      <li>Augmenting the Real World (AR Development): designed and supported lab sessions (C#, Unity)</li>
    </ul>
  </div>
  
  <br>
  <div class="entry">
    <b>Research Supervision</b>
    <p><strong>University of Bristol</strong>   <span style="float: right;">Bristol, UK &nbsp;&nbsp; Jun. 2022 - Present</span></p>
    <ul>
      <li>Mentored <strong>twelve</strong> postgraduate students and <strong>six</strong> undergraduate students through their final-year thesis and internship projects, providing guidance on research methodologies and experimental design.</li>
      <li>Topics included video quality assessment, video compression, and image processing.</li>
    </ul>
  </div>
</div>
		
			
				</div>
     
            	
            <hr />
			
			
			<div class="container-md">
  <h2 id="projects">Awards and Honours</h2>
  <ul>
    <li><font style="color:red">1st Place</font> in Video Perception at the 6th Challenge on Learned Image Compression in DCC 2024</li><a href="./CLIC 2024 BVI_VQA.pdf">(award letter)</a></li>
    <li><font style="color:red">1st Winner</font> in the 3rd Practical End-to-End Image/Video Compression Challenge in IEEE MMSP 2024</li>
    <li><font style="color:red">3rd Place</font> in the 5th AIM Challenge on Efficient Video Super-Resolution in ECCV 2024</li>
    <li><font style="color:red">The FIRST</font> Prize in IEEE/CVF WACV 2023 HDR VQM Grand Challenge hosted by <a href="https://sites.google.com/view/wacv2023-workshop-quality-va/competition?authuser=0">Amazon Prime Video</a> <a href="./WACV.pdf">(award letter)</a></li>
    <li>UKRI MyWorld Strength Research Students Awards Scheme Scholarship.</li>
    <li>PhD is funded by the <a href="https://www.amazon.science/research-awards/program-updates/79-amazon-research-awards-recipients-announced">Amazon Research Awards 2022</a></li>
    <li><font style="color:red">Top Six</font> in the 5th Challenge on Learned Image Compression in IEEE/CVF CVPR 2022</li>
    <li><font style="color:red">Second Prize</font> in the Grand Challenge on NN-based Video Coding in IEEE ISCAS 2022</li>
    <li>University of Bristol - Bristol PLUS Award</li>
  </ul>
  <br>
</div>
       
            	
            <hr />
			
			
<div class="container-md">
  <h2 id="projects">Professional Activities</h2>
  <ul>
    <li>Reviewer for the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS) 2024</li>
    <li>Reviewer for the International Conference on Learning Representations (ICLR) 2025</li>
    <li>Reviewer for the European Conference on Computer Vision (ECCV) 2024</li>
    <li>Reviewer for the Picture Coding Symposium (PCS) 2024</li>
    <li>Reviewer of IEEE Transactions on Image Processing (TIP)</li>
    <li>Reviewer of IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</li>
    <li>Reviewer of IEEE Transactions on Broadcasting</li>
    <li>Reviewer of Pattern Recognition Letters</li>
    <li>Reviewer of IEEE Transactions on Artificial Intelligence (T-AI)</li>
    <li>Reviewer of IEEE Signal Processing Letters</li>
  </ul>
  <br>
</div>


</ol>

             
		
            	
            <hr />
			
			

			
			<div class="container-md">
  <h2 id="publications">Publications (<a href="https://scholar.google.com/citations?user=kWY2QHoAAAAJ&hl">Google Scholar</a>)</h2>
  <br>

  <b>arXiv Papers</b>
  <ol start="1">
    <li>
      <b><a href="https://arxiv.org/abs/2408.07171">BVI-UGC: A Video Quality Database for User-Generated Content Transcoding</a></b>
      <br>
      Z. Qi, <b>C. Feng</b>, F. Zhang, X. Xu, S. Liu, and D. R. Bull
      <br>
      <i>arXiv:2408.07171</i>, 2024
      <br>
      <a href="https://zihaoq1.github.io/BVI-UGC/">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://arxiv.org/abs/2207.08634">Enhancing HDR Video Compression through CNN-based Effective Bit Depth Adaptation</a></b>
      <br>
      <b>C. Feng</b>, Z. Qi, D. Danier, F. Zhang, X. Xu, S. Liu, and D. R. Bull
      <br>
      <i>arXiv:2207.08634</i>, 2022
      <br>
      <a href="HDR-BD/index.htm">[project]</a>
      <p></p>
    </li>
  </ol>

  <b>Journal Papers</b>
  <ol start="1">
    <li>
      <b><a href="https://chenfeng-bristol.github.io/CNN-PP/index.htm">Video Compression with CNN-based Postprocessing</a></b>
      <br>
      F. Zhang, D. Ma, <b>C. Feng</b>, and D. R. Bull
      <br>
      <i>IEEE MultiMedia</i>, 2021
      <br>
      <a href="https://arxiv.org/abs/2009.07583">[paper]</a> | <a href="CNN-PP/index.htm">[project]</a>
      <p></p>
    </li>
  </ol>

  <b>Conference Papers</b>
  <ol start="1">
        <li>
      <b><a href="https://chenfeng-bristol.github.io/MVAD/index.htm">MVAD: A Multiple Visual Artifact Detector for Video Streaming</a></b>
      <br>
      <b>C. Feng</b>, D. Danier, F. Zhang, and D. R. Bull
      <br>
      <i>IEEE/CVF WACV</i>, 2025
      <br>
      <a href="https://chenfeng-bristol.github.io/MVAD/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://openaccess.thecvf.com/content/WACV2024/html/Feng_RankDVQA_Deep_VQA_Based_on_Ranking-Inspired_Hybrid_Training_WACV_2024_paper.html">RankDVQA: Deep VQA based on Ranking-inspired Hybrid Training</a></b>
      <br>
      <b>C. Feng</b>, D. Danier, F. Zhang, and D. R. Bull
      <br>
      <i>IEEE/CVF WACV</i>, 2024
      <br>
      <a href="RankDVQA/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://arxiv.org/abs/2404.09571">MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution</a></b>
      <br>
      Y. Jiang, <b>C. Feng</b>, F. Zhang, and D. Bull
      <br>
      <i>IEEE/CVF ECCV</i>, 2024
      <br>
      <a href="https://arxiv.org/abs/2404.09571">[paper]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://arxiv.org/abs/2405.08621">RMT-BVQA: Recurrent Memory Transformer-based Blind VQA for Enhanced Video Content</a></b>
      <br>
      Tianhao Peng*, <b>C. Feng*</b>, D. Danier, F. Zhang, and D. Bull
      <br>
      <i>ECCV Workshop in AIM</i>, 2024
      <br>
      <a href="https://arxiv.org/abs/2405.08621">[paper]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://chenfeng-bristol.github.io/BVI-Artefact/index.htm">BVI-Artefact: An Artefact Detection Benchmark Dataset for Streamed Videos</a></b>
      <br>
      <b>C. Feng</b>, D. Danier, F. Zhang, and D. Bull
      <br>
      <i>Picture Coding Symposium (PCS)</i>, 2024 (Oral Session)
      <br>
      <a href="https://arxiv.org/pdf/2312.08859.pdf">[paper]</a> | <a href="BVI-Artefact/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://chenfeng-bristol.github.io/RankDVQA-mini/index.htm">RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality Assessment</a></b>
      <br>
      <b>C. Feng</b>, D. Danier, H. Wang, F. Zhang, and D. R. Bull
      <br>
      <i>Picture Coding Symposium (PCS)</i>, 2024 (Oral Session)
      <br>
      <a href="https://arxiv.org/pdf/2312.08864.pdf">[paper]</a> | <a href="RankDVQA-mini/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://zihaoq1.github.io/FRUGC/">Full-reference Video Quality Assessment for User Generated Content Transcoding</a></b>
      <br>
      Z. Qi, <b>C. Feng</b>, D. Danier, F. Zhang, X. Xu, S. Liu, and D. R. Bull
      <br>
      <i>Picture Coding Symposium (PCS)</i>, 2024 (Oral Session)
      <br>
      <a href="https://arxiv.org/pdf/2312.12317.pdf">[paper]</a> | <a href="https://zihaoq1.github.io/FRUGC/">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://chenfeng-bristol.github.io/ViSTRA3/index.htm">ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing</a></b>
      <br>
      <b>C. Feng</b>, D. Danier, C. Tan, F. Zhang, and D. Bull
      <br>
      <i>IEEE ISCAS</i>, 2022
      <br>
      <a href="https://arxiv.org/abs/2111.15536">[paper]</a> | <a href="ViSTRA3/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://chenfeng-bristol.github.io/MFCNN-PP/index.htm">Enhancing VVC with Deep Learning based Multi-Frame Post-Processing</a></b>
      <br>
      D. Danier, <b>C. Feng</b>, F. Zhang, and D. Bull
      <br>
      <i>CVPR 5th Challenge on Learned Image Compression</i>, 2022
      <br>
      <a href="https://arxiv.org/abs/2205.09458">[paper]</a> | <a href="MFCNN-PP/index.htm">[project]</a>
      <p></p>
    </li>
    <li>
      <b><a href="https://ieeexplore.ieee.org/document/9102912">Enhancing VVC through CNN-based Post-Processing</a></b>
      <br>
      F. Zhang, <b>C. Feng</b>, and D. Bull
      <br>
      <i>IEEE ICME</i>, 2020
      <br>
      <a href="https://ieeexplore.ieee.org/document/9102912">[paper]</a> | <a href="CNN-PP/index.htm">[project]</a>
      <p></p>
    </li>
  </ol>
  <hr />
</div>

			
	
<!--             <div class="container-md">
                <h2 id="projects">Publication</h2>


<br>






<b>arXiv Papers</b>


<ol start="1">
<li> BVI-Artefact: An Artefact Detection Benchmark Dataset for Streamed Videos. <A HREF="https://arxiv.org/pdf/2312.08859.pdf">[paper]</A><a href="BVI-Artefact/index.htm">[project]</a>
<br>
C. Feng, D. Danier, F. Zhang, and D. R. Bull, arXiv:2312.08859, 2023
<br>
<p>
<li> RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality Assessment. <A HREF="https://arxiv.org/pdf/2312.08864.pdf">[paper]</A><a href="RankDVQA-mini/index.htm">[project]</a>
<br>
C. Feng, D. Danier, H. Wang, F. Zhang, and D. R. Bull, arXiv:2312.08864, 2023
<br>
<p>
<li> Full-reference Video Quality Assessment for User Generated Content Transcoding. <A HREF="https://arxiv.org/pdf/2312.12317.pdf">[paper]</A><a href="https://zihaoq1.github.io/FRUGC/">[project]</a>
<br>
Z. Qi, C. Feng, D. Danier, F. Zhang, X. Xu, S. Liu, and D. R. Bull, arXiv:2312.12317, 2023
<br>
<p>
<li> Enhancing HDR Video Compression through CNN-based Effective Bit Depth Adaptation. <A HREF="https://arxiv.org/abs/2207.08634">[paper]</A><a href="HDR-BD/index.htm">[project]</a>
<br>
C. Feng, F. Zhang, and D. R. Bull, arXiv:2202.08595, 2022
<br>
<p>

<li> Enhancing VVC with Deep Learning based Multi-Frame Post-Processing. <A HREF="https://arxiv.org/abs/2205.09458">[paper]</A><a href="MFCNN-PP/index.htm">[project]</a>
<br>
C. Feng, F. Zhang, and D. R. Bull, arXiv:2202.08595, 2022
<br>
<p>


</ol>



<b>Journal Papers</b>

<ol start="1">

<li> Video Compression with CNN-based Post Processing. <A HREF="https://arxiv.org/abs/2009.07583">[paper]</A><a href="CNN-PP/index.htm">[project]</a>
<br>
F. Zhang, D. Ma, C. Feng and D. R. Bull, <em>IEEE MultiMedia Magazine</em>, 2020.
<br>
<p>


</ol>



<b>Conference papers</b>

<ol start="1">
<li> RankDVQA: Deep VQA based on Ranking-inspired Hybrid Training. <A HREF="https://openaccess.thecvf.com/content/WACV2024/html/Feng_RankDVQA_Deep_VQA_Based_on_Ranking-Inspired_Hybrid_Training_WACV_2024_paper.html">[paper]</A><a href="RankDVQA/index.htm">[project]</a>
<br>
C. Feng, D. Danier, F. Zhang, and D. R. Bull, accepted by IEEE/CVF WACV, 2024
<br>
<p>
<li>ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing. <A HREF="https://arxiv.org/abs/2111.15536">[paper]</A><a href="ViSTRA3/index.htm">[project]</a>
<br>
C. Feng, D. Danier, C. Tan, F. Zhang, D. Bull, accepted by IEEE ISCAS, 2022
<br>
<p>
<li>Enhancing VVC through CNN-based Post-Processing. <A HREF="https://ieeexplore.ieee.org/document/9102912">[paper]</A><a href="CNN-PP/index.htm">[project]</a>
<br>
F. Zhang, C. Feng and D. Bull, <em>ICME,</em> 2020.
<br>
<p>


</ol>
             
            	
            <hr /> -->

		


<div class="container-md">
                <h2 id="projects">Technical Skills</h2>
				<ul>
					<li>Programming: Python, Matlab, C++, C\#, Java, Assembly, Statistical Analysis.</li>
					<li>PyTorch, Tensorflow, Generative Models, CNNs, GAN, Transformer, Autoencoders.</li>
					<li>Tools: Unity(VR\&AR Development), Git, Docker, IDEs, LaTeX, Raspberry Pi, LabVIEW</li>
		

				</ul>

<br>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11440198; 
var sc_invisible=0; 
var sc_security="e7e7610e"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11440198/0/e7e7610e/0/" alt="web
analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
		
            </div>
            <hr />
        </div>
    </div>
	<script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>	
<script>
      $(function() {
	  var olderNews = $("#olderNews");
	  olderNews.hide();
	  $("#showOlderNews").click(function() {
	      olderNews.show(1000);
	  });
      });
</script>

<script>
      $(function() {
	  var olderNews = $("#olderProjects");
	  olderNews.hide();
	  $("#showOlderProjects").click(function() {
	      olderNews.show(1000);
	  });
      });
</script>

</body>

</html>
